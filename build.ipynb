{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! don't use this unless you're me\n",
    "yo_mama_path = \"/Volumes/YoMama/Rizzerator/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skylerestavillo/opt/anaconda3/envs/rizz/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import soundfile as sf\n",
    "import json\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "\"\"\"\n",
    "if librosa.__version__ != '0.6.2':\n",
    "    os.system('pip3 install librosa==0.6.2')\n",
    "    import librosa\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# model imports\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.nn import CosineSimilarity\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loop through .wav files and their corresponding .json files\n",
    "data_path_caller = yo_mama_path + \"data/audio/caller/\"\n",
    "data_path_agent = yo_mama_path + \"data/audio/agent/\"\n",
    "transcript_path = yo_mama_path + \"data/transcript/\"\n",
    "\n",
    "all_wav_files_caller = [f for f in os.listdir(data_path_caller) if f.endswith(\".wav\")]\n",
    "\n",
    "# Select 3 random samples\n",
    "# ! Increase later, keeping this low to test\n",
    "wav_files_caller = random.sample(all_wav_files_caller, 6)\n",
    "\n",
    "# Use the same filenames for the agent audio files\n",
    "wav_files_agent = wav_files_caller\n",
    "\n",
    "json_files = [f.replace(\".wav\", \".json\") for f in wav_files_caller]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make function to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the current one below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "from scipy.signal import lfilter, butter # do i need this? i can't remember\n",
    "from scipy.signal import find_peaks\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "# Function to extract acoustic features\n",
    "def extract_acoustic_features(audio_file, start_time, end_time, window_size=0.1, hop_size=0.05):\n",
    "    # Load the audio file with a specified start and end time\n",
    "    y, sr = librosa.load(audio_file, sr=None, offset=start_time, duration=end_time - start_time)\n",
    "    \n",
    "    # If the audio file is empty, return None\n",
    "    if len(y) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Set the target sample rate and resample the audio if necessary\n",
    "    target_sr = 4000\n",
    "    if sr > target_sr:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "        sr = target_sr\n",
    "\n",
    "    # Calculate window and hop lengths based on sample rate and provided parameters\n",
    "    window_length = int(window_size * sr)\n",
    "    hop_length = int(hop_size * sr)\n",
    "\n",
    "    # Initialize lists to store feature values\n",
    "    zcr_values = []\n",
    "    energy_values = []\n",
    "    spectral_centroid_values = []\n",
    "    spectral_spread_values = []\n",
    "    pitch_values = []\n",
    "    spectral_entropy_values = []\n",
    "    mfcc_values = []\n",
    "    formant_values = []\n",
    "\n",
    "    # Calculate spectral entropy and handle potential errors\n",
    "    def spectral_entropy(spectrogram, normalize=True):\n",
    "        psd_norm = np.divide(spectrogram, spectrogram.sum(axis=0, keepdims=True))\n",
    "        log_psd_norm = -np.log2(psd_norm, where=(psd_norm > 0))\n",
    "        entropy = np.sum(psd_norm * log_psd_norm, axis=0)\n",
    "        if normalize:\n",
    "            entropy /= np.log2(psd_norm.shape[0])\n",
    "        return entropy \n",
    "    \n",
    "\n",
    "    def formants(y, sr, n_formants=5, order=16):\n",
    "        epsilon = 1e-8  # Add a small constant value to avoid zero-valued samples\n",
    "        y = y + epsilon\n",
    "        lpc = librosa.lpc(y, order=order)\n",
    "        \n",
    "        if not np.isfinite(lpc).all():  # Check if lpc contains any inf or nan values\n",
    "            return [], []  # Return empty lists if inf or nan values are present\n",
    "\n",
    "        roots = np.roots(lpc)\n",
    "        roots = roots[np.imag(roots) >= 0]\n",
    "        angles = np.angle(roots)\n",
    "        freqs = sorted(sr * angles / (2 * np.pi))\n",
    "        bandwidths = -0.5 * sr * np.log(np.abs(roots))\n",
    "        freqs = freqs[:n_formants]\n",
    "        bandwidths = bandwidths[:n_formants]\n",
    "        return freqs, bandwidths\n",
    "    \n",
    "\n",
    "    # Iterate through the audio signal using a sliding window\n",
    "    for start in range(0, len(y) - window_length, hop_length):\n",
    "        # Extract the current window\n",
    "        end = start + window_length\n",
    "        y_window = y[start:end]\n",
    "\n",
    "        # Calculate various features for the current window\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y_window))\n",
    "        energy = np.mean(librosa.feature.rms(y=y_window))\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y_window, sr=sr))\n",
    "        spectral_spread = np.mean(librosa.feature.spectral_bandwidth(y=y_window, sr=sr))\n",
    "\n",
    "        # Calculate pitch and handle potential errors\n",
    "         # ! Causing significant errors in pitch and spectral entropy feature extraction. Gives nan values all the time\n",
    "        try:\n",
    "            pitch, harmonic_strength, _ = librosa.pyin(y_window, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
    "            mean_pitch = np.mean(pitch)\n",
    "        except librosa.util.exceptions.ParameterError:\n",
    "            pitch = np.nan\n",
    "\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            spectrogram = np.abs(librosa.stft(y_window, n_fft=2048, hop_length=512))\n",
    "            entropy = np.mean(spectral_entropy(spectrogram))\n",
    "            spectral_entropy_values.append(entropy)\n",
    "        except librosa.util.exceptions.ParameterError:\n",
    "            spectral_entropy = np.nan\n",
    "\n",
    "        # Calculate MFCCs for the current window\n",
    "        mfcc = np.mean(librosa.feature.mfcc(y=y_window, sr=sr, n_mfcc=13), axis=1)\n",
    "\n",
    "        # Calculate formant values for the current window (Updated)\n",
    "        formant_frequencies, formant_bandwidths = formants(y_window, sr)\n",
    "        formant_values.append(formant_frequencies)\n",
    "\n",
    "        # Append feature values to the corresponding lists\n",
    "        zcr_values.append(zcr)\n",
    "        energy_values.append(energy)\n",
    "        spectral_centroid_values.append(spectral_centroid)\n",
    "        spectral_spread_values.append(spectral_spread)\n",
    "        pitch_values.append(pitch)\n",
    "        spectral_entropy_values.append(spectral_entropy)\n",
    "        mfcc_values.append(mfcc)\n",
    "\n",
    "    # Calculate speaking rate by counting the number of audio chunks\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    audio_chunks = split_on_silence(audio, min_silence_len=200, silence_thresh=-36)\n",
    "    speaking_rate = len(audio_chunks) / (end_time - start_time)\n",
    "\n",
    "    # Calculate jitter and shimmer\n",
    "    pitch_contour = librosa.yin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7')) \n",
    "    jitter = np.mean(np.abs(np.diff(pitch_contour)) / np.mean(pitch_contour)) \n",
    "    shimmer = np.mean(np.abs(np.diff(librosa.feature.rms(y=y))) / np.mean(librosa.feature.rms(y=y)))\n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "    # Combine all extracted features into a single list\n",
    "    features = [zcr_values, energy_values, spectral_centroid_values, spectral_spread_values, pitch_values, spectral_entropy_values, mfcc_values, speaking_rate, jitter, shimmer, formant_values]\n",
    "    # Convert the list of features to a NumPy array\n",
    "    features_array = np.array(features)\n",
    "\n",
    "    return features_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Function to extract acoustic features\n",
    "def extract_acoustic_features(audio_file, start_time, end_time, window_size=0.1, hop_size=0.05):\n",
    "    y, sr = librosa.load(audio_file, sr=None, offset=start_time, duration=end_time - start_time)\n",
    "    \n",
    "    # Return nothing if length of audio file is 0\n",
    "    if len(y) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Resample the audio if necessary\n",
    "    target_sr = 4000\n",
    "    if sr > target_sr:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "        sr = target_sr\n",
    "\n",
    "    # Calculate window and hop lengths based on sample rate and provided parameters\n",
    "    window_length = int(window_size * sr)\n",
    "    hop_length = int(hop_size * sr)\n",
    "\n",
    "    # Initialize lists to store feature values\n",
    "    zcr_values = []\n",
    "    energy_values = []\n",
    "    spectral_centroid_values = []\n",
    "    spectral_spread_values = []\n",
    "    pitch_values = []\n",
    "    spectral_entropy_values = []\n",
    "\n",
    "    # Iterate through the audio signal using a sliding window\n",
    "    for start in range(0, len(y) - window_length, hop_length):\n",
    "        end = start + window_length\n",
    "        # Extract the current window\n",
    "        y_window = y[start:end]\n",
    "\n",
    "        # Calculate various features for the current window\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y_window))\n",
    "        energy = np.mean(librosa.feature.rms(y=y_window))\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y_window, sr=sr))\n",
    "        spectral_spread = np.mean(librosa.feature.spectral_bandwidth(y=y_window, sr=sr))\n",
    "\n",
    "        # Calculate pitch and handle potential errors\n",
    "        # ! Causing significant errors in pitch and spectral entropy feature extraction. Gives nan values all the time\n",
    "        try:\n",
    "            pitch = np.mean(librosa.feature.tonnetz(y=y_window, sr=sr))\n",
    "        except librosa.util.exceptions.ParameterError:\n",
    "            pitch = np.nan\n",
    "\n",
    "        try:\n",
    "            spectral_entropy = np.mean(librosa.feature.spectral_contrast(y=y_window, sr=sr))\n",
    "        except librosa.util.exceptions.ParameterError:\n",
    "            spectral_entropy = np.nan\n",
    "\n",
    "        # Append feature values to the corresponding lists\n",
    "        zcr_values.append(zcr)\n",
    "        energy_values.append(energy)\n",
    "        spectral_centroid_values.append(spectral_centroid)\n",
    "        spectral_spread_values.append(spectral_spread)\n",
    "        pitch_values.append(pitch)\n",
    "        spectral_entropy_values.append(spectral_entropy)\n",
    "\n",
    "    # Combine all extracted features into a single list\n",
    "    features = [zcr_values, energy_values, spectral_centroid_values, spectral_spread_values, pitch_values, spectral_entropy_values]\n",
    "    features_array = np.array(features)\n",
    "\n",
    "    return features_array\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe audio with Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Transcribe audio using Whisper\n",
    "model = whisper.load_model(\"base.en\")\n",
    "\n",
    "def transcribe_audio_whisper(audio_file, start_time, end_time):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file, sr=None, offset=start_time, duration=end_time - start_time)\n",
    "    \n",
    "    # Save the segmented audio to a temporary file\n",
    "    temp_file = yo_mama_path + \"temp.wav\"\n",
    "    sf.write(temp_file, y, sr)\n",
    "    \n",
    "    # !Implement actual Whisper transcription here\n",
    "    transcript = model.transcribe(temp_file)\n",
    "    \n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    return transcript"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pretrained models for fine tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained Word2Vec model\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Show all available models in gensim-data\n",
    "print(list(api.info()['models'].keys()))\n",
    "\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "#word2vec_model = Word2Vec(dataset)  # train w2v model\n",
    "\n",
    "#word2vec_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "# For saving and loading Word2Vec model\n",
    "word2vec_model_path = os.path.join(yo_mama_path, 'word2vec_model')\n",
    "\n",
    "# Save the model to a file\n",
    "word2vec_model.save(yo_mama_path + 'word2vec_model')\n",
    "\n",
    "# Load the saved model\n",
    "#saved_w2v = gensim.models.Word2Vec.load('word2vec_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe model\n",
    "glove_file = \"glove.6B/glove.6B.100d.txt\"\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
    "\n",
    "# Save the GloVe model in the word2vec format\n",
    "word2vec_glove_file = yo_mama_path + \"glove_word2vec.txt\"\n",
    "#glove_model.save_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Cosine similarity for comparing embeddings\n",
    "cosine_similarity = CosineSimilarity(dim=1)\n",
    "\n",
    "# Save the model and tokenizer to a directory\n",
    "#bert_model.save_pretrained('bert_model')\n",
    "#bert_tokenizer.save_pretrained('bert_tokenizer')\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "#saved_model = BertModel.from_pretrained('bert_model')\n",
    "#saved_tokenizer = BertTokenizer.from_pretrained('bert_tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to extract linguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Volumes/YoMama/Rizzerator/model/...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\"\"\"\n",
    "nltk.download('popular')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "nltk.data.path.append(yo_mama_path)\n",
    "nltk.download('popular', download_dir=yo_mama_path)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt', download_dir=yo_mama_path)\n",
    "nltk.download('stopwords', download_dir=yo_mama_path)\n",
    "nltk.download('wordnet', download_dir=yo_mama_path)\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Extract linguistic features using Word2Vec, GloVe, and BERT \n",
    "def extract_linguistic_features(transcript):\n",
    "    # Word2Vec features\n",
    "    # Preprocess the transcript to tokenize and clean the text\n",
    "    words = preprocess(transcript)\n",
    "    # Initialize an empty list to store Word2Vec features\n",
    "    word2vec_features = []\n",
    "\n",
    "    # Iterate over each word in the preprocessed transcript\n",
    "    for word in words:\n",
    "        # If the word is present in the Word2Vec model, append its vector representation to the word2vec_features list\n",
    "        if word in word2vec_model:\n",
    "            word2vec_features.append(word2vec_model[word])\n",
    "        # If the word is not present in the Word2Vec model, append a zero vector of the same dimension as the model's vectors\n",
    "        else:\n",
    "            word2vec_features.append(np.zeros(word2vec_model.vector_size))\n",
    "\n",
    "    # Calculate the mean of the Word2Vec features along axis 0 (i.e., column-wise mean)\n",
    "    word2vec_features = np.mean(word2vec_features, axis=0)\n",
    "\n",
    "    # GloVe features\n",
    "    # Calculate the mean GloVe vector for words in the transcript that are present in the GloVe model\n",
    "    glove_features = np.mean([glove_model[word] for word in transcript.split() if word in glove_model], axis=0)\n",
    "\n",
    "    # BERT features\n",
    "    # Ensure no gradients are calculated for this operation (speeds up computation and reduces memory usage)\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the transcript using the BERT tokenizer and convert it to a tensor, then add a batch dimension\n",
    "        input_ids = torch.tensor(bert_tokenizer.encode(transcript)).unsqueeze(0)\n",
    "        # Pass the input IDs through the BERT model to get the output\n",
    "        bert_output = bert_model(input_ids)\n",
    "        # Extract the mean of the last hidden state tensor along dimension 1 (i.e., average the hidden states of all tokens), then remove the batch dimension\n",
    "        # ! NGL, I have no idea what this does\n",
    "        # What is happening here? \n",
    "        # The BERT model outputs a tuple of 2 tensors: the last hidden state and the pooler output\n",
    "        # The last hidden state is a tensor of shape (batch_size, sequence_length, hidden_size)\n",
    "        # The pooler output is a tensor of shape (batch_size, hidden_size)\n",
    "        # # The mean of the last hidden state tensor along dimension 1 is a tensor of shape (batch_size, hidden_size)\n",
    "        # # The squeeze() method removes the batch dimension\n",
    "        bert_features = bert_output.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "    # Return the extracted Word2Vec, GloVe, and BERT features\n",
    "    return word2vec_features, glove_features, bert_features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform acoustic feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current problems:\n",
    " - Calculating the \"mean of an empty slice\" in some cases. What's going on there?\n",
    " - RuntimeWarning: invalid value encountered in double_scalars\n",
    " - Whisper transcript sometimes not lining up correctly with the provided transcript. Should we just use the provided transcript?\n",
    " - Sometimes the provided transcript is \"[noise]\", other times \"[noise][noise]\", and other times \"[noise][noise][noise]\". These all need to be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skylerestavillo/opt/anaconda3/envs/rizz/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/skylerestavillo/opt/anaconda3/envs/rizz/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/skylerestavillo/opt/anaconda3/envs/rizz/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/var/folders/16/dzn2rycx0bn7yhztfsfmllsc0000gn/T/ipykernel_6390/2379333889.py:38: RuntimeWarning: invalid value encountered in divide\n",
      "  psd_norm = np.divide(spectrogram, spectrogram.sum(axis=0, keepdims=True))\n",
      "/var/folders/16/dzn2rycx0bn7yhztfsfmllsc0000gn/T/ipykernel_6390/2379333889.py:58: RuntimeWarning: divide by zero encountered in log\n",
      "  bandwidths = -0.5 * sr * np.log(np.abs(roots))\n",
      "/var/folders/16/dzn2rycx0bn7yhztfsfmllsc0000gn/T/ipykernel_6390/2379333889.py:117: RuntimeWarning: invalid value encountered in divide\n",
      "  shimmer = np.mean(np.abs(np.diff(librosa.feature.rms(y=y))) / np.mean(librosa.feature.rms(y=y)))\n",
      "/var/folders/16/dzn2rycx0bn7yhztfsfmllsc0000gn/T/ipykernel_6390/2379333889.py:40: RuntimeWarning: invalid value encountered in multiply\n",
      "  entropy = np.sum(psd_norm * log_psd_norm, axis=0)\n",
      "/Users/skylerestavillo/opt/anaconda3/envs/rizz/lib/python3.9/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing: 1\n"
     ]
    }
   ],
   "source": [
    "# 4. Extract acoustic features and create data\n",
    "data = []\n",
    "count = 0\n",
    "\n",
    "for wav_file_caller, wav_file_agent, json_file in zip(wav_files_caller, wav_files_agent, json_files):\n",
    "    with open(transcript_path + json_file) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    human_transcripts = [entry[\"human_transcript\"] for entry in json_data if (entry[\"human_transcript\"] != \"[noise]\" or entry[\"human_transcript\"] != \"[noise][noise]\" or entry[\"human_transcript\"] != \"[noise][noise][noise]\")]\n",
    "    emotion_scores = [entry[\"emotion\"] for entry in json_data if (entry[\"human_transcript\"] != \"[noise]\" or entry[\"human_transcript\"] != \"[noise][noise]\" or entry[\"human_transcript\"] != \"[noise][noise][noise]\")]\n",
    "    offset_durations = [(entry[\"offset_ms\"], entry[\"duration_ms\"]) for entry in json_data if (entry[\"human_transcript\"] != \"[noise]\" or entry[\"human_transcript\"] != \"[noise][noise]\" or entry[\"human_transcript\"] != \"[noise][noise][noise]\")]\n",
    "    speaker_roles = [entry[\"speaker_role\"] for entry in json_data if (entry[\"human_transcript\"] != \"[noise]\" or entry[\"human_transcript\"] != \"[noise][noise]\" or entry[\"human_transcript\"] != \"[noise][noise][noise]\")]\n",
    "\n",
    "    # Extract acoustic features and linguistic features for each human transcript\n",
    "    for transcript, emotion_score, (offset_ms, duration_ms), speaker_role in zip(human_transcripts, emotion_scores, offset_durations, speaker_roles):\n",
    "        start_time = offset_ms / 1000\n",
    "        end_time = (offset_ms + duration_ms) / 1000\n",
    "\n",
    "        # Change file path according to speaker role\n",
    "        if speaker_role == \"caller\":\n",
    "            audio_file = data_path_caller + wav_file_caller\n",
    "        else:\n",
    "            audio_file = data_path_agent + wav_file_agent\n",
    "        \n",
    "        acoustic_features = extract_acoustic_features(audio_file, start_time, end_time)\n",
    "\n",
    "        if acoustic_features is not None:\n",
    "            # ! The issue with the whisper transcript is that its not as good as the human_transcript\n",
    "            \"\"\"\n",
    "            whisper_transcript = transcribe_audio_whisper(audio_file, start_time, end_time)\n",
    "            print(whisper_transcript) # Printing just to see what's going on\n",
    "            whisper_text = whisper_transcript['text'] # Extract the text portion from the dictionary\n",
    "            \"\"\"\n",
    "            whisper_text = transcript\n",
    "            word2vec_features, glove_features, bert_features = extract_linguistic_features(whisper_text)\n",
    "\n",
    "            row = (audio_file, speaker_role, transcript, word2vec_features, glove_features, bert_features, *acoustic_features, emotion_score)\n",
    "            data.append(row)\n",
    "    count += 1  # Increment the counter\n",
    "    print(f\"Finished processing: {count}\")  # Print the current progress"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinAlgError                               Traceback (most recent call last)\n",
    "Cell In[25], line 25\n",
    "     22 else:\n",
    "     23     audio_file = data_path_agent + wav_file_agent\n",
    "---> 25 acoustic_features = extract_acoustic_features(audio_file, start_time, end_time)\n",
    "     27 if acoustic_features is not None:\n",
    "     28     # ! The issue with the whisper transcript is that its not as good as the human_transcript\n",
    "     29     \"\"\"\n",
    "     30     whisper_transcript = transcribe_audio_whisper(audio_file, start_time, end_time)\n",
    "     31     print(whisper_transcript) # Printing just to see what's going on\n",
    "     32     whisper_text = whisper_transcript['text'] # Extract the text portion from the dictionary\n",
    "     33     \"\"\"\n",
    "\n",
    "Cell In[13], line 91, in extract_acoustic_features(audio_file, start_time, end_time, window_size, hop_size)\n",
    "     88 mfcc = np.mean(librosa.feature.mfcc(y=y_window, sr=sr, n_mfcc=13), axis=1)\n",
    "     90 # Calculate formant values for the current window (Updated)\n",
    "---> 91 formant_frequencies, formant_bandwidths = formants(y_window, sr)\n",
    "     92 formant_values.append(formant_frequencies)\n",
    "     94 # Append feature values to the corresponding lists\n",
    "\n",
    "Cell In[13], line 48, in extract_acoustic_features..formants(y, sr, n_formants, order)\n",
    "     46 def formants(y, sr, n_formants=5, order=16):\n",
    "     47     lpc = librosa.lpc(y, order=order)\n",
    "---> 48     roots = np.roots(lpc)\n",
    "...\n",
    "    207 for a in arrays:\n",
    "    208     if not isfinite(a).all():\n",
    "--> 209         raise LinAlgError(\"Array must not contain infs or NaNs\")\n",
    "\n",
    "LinAlgError: Array must not contain infs or NaNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Export data to a CSV file\n",
    "column_names = [\"Audio\", \"SpeakerRole\", \"Transcript\", \"Word2Vec\", \"GloVe\", \"BERT\", \"ZCR\", \"Energy\", \"SpectralCentroid\", \"SpectralSpread\", \"Pitch\", \"SpectralEntropy\", \"MFCC\", \"SpeakingRate\", \"Jitter\", \"Shimmer\", \"FormantValues\", \"EmotionScore\"]\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "df.to_csv(yo_mama_path + \"output.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save feature arrays as NumPy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(yo_mama_path + \"zcr_values.npy\", np.vstack(df[\"ZCR\"].values))\n",
    "np.save(yo_mama_path + \"energy_values.npy\", np.vstack(df[\"Energy\"].values))\n",
    "np.save(yo_mama_path + \"spectral_centroid_values.npy\", np.vstack(df[\"SpectralCentroid\"].values))\n",
    "np.save(yo_mama_path + \"spectral_spread_values.npy\", np.vstack(df[\"SpectralSpread\"].values))\n",
    "np.save(yo_mama_path + \"pitch_values.npy\", np.vstack(df[\"Pitch\"].values))\n",
    "np.save(yo_mama_path + \"spectral_entropy_values.npy\", np.vstack(df[\"SpectralEntropy\"].values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rizz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "feb107a098849b32906f855eda7b629f34d4fe10c2185e28497f1608518d1332"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
